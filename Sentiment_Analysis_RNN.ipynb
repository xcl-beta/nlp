{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic RNN implementation. \n",
    "\n",
    "note: focus is to lean to use pack_padded_sequence and pad_packed_sequence to fit variant lengths of input into RNN.\n",
    "\n",
    "followed refererences : \n",
    "https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ5SE1D2mFMs"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PAG915JevD3i"
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random_state = random.seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if \n",
    "    torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm') # <1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "AEjFcr_aJgIr"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_iter, test_iter = IMDB(\n",
    "    split=('train', 'test')) #<1>\n",
    "\n",
    "train_dataset = list(train_iter) #<2>\n",
    "test_dataset  = list(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.70)\n",
    "train_data, valid_data = \\\n",
    "    random_split(train_dataset, \n",
    "        [num_train, \n",
    "         len(train_dataset) - num_train]) # <3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1615841915453,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "7IOq1ufhq01Q",
    "outputId": "ffa9f08a-66ac-4119-a257-c4fd2c035ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 7500 25000\n",
      "neg\n",
      "I missed the beginning but I did see most of it. A friend got it on DVD in the cheap room at FYE.<br /><br />The skits are all very short, and yet most of them are still too long. The majority of them, they seem to have forgotten to have something funny! Quite a lot of racist/sexist/\"homophobic\" humor in it, skits based on stereotypes, or skits which use racist terms for people.<br /><br />I'm trying to remember anything I thought was funny in it, and I'm having trouble.... The logo for the Tunnel Vision network is a lipsticked mouth with an eyeball in it. The mouth opens and closes over the eye like eyelids. Kind of creepy.<br /><br />What a disappointment. Most of the actors went on to better things, and it's lucky this bomb didn't hold them back.\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data), len(test_dataset))\n",
    "# out:17500 7500 25000\n",
    "\n",
    "data_index = 21\n",
    "print(train_data[data_index][0])\n",
    "# out: (your results may vary)\n",
    "#   pos\n",
    "\n",
    "print(train_data[data_index][1])\n",
    "# out: (your results may vary)\n",
    "# ['This', 'film', 'moved', 'me', 'beyond', 'comprehension', ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pre-trained embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embed_len = 200\n",
    "global_vectors = GloVe(name='6B', dim=embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2661,  0.2182, -0.1100,  ..., -0.1198, -0.1916, -0.1352],\n",
       "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
       "        [ 0.1815,  0.2663,  0.0550,  ...,  0.5375,  0.3151,  0.0162],\n",
       "        [ 0.0367,  0.1989, -0.0930,  ..., -0.0133, -0.0039,  0.7128],\n",
       "        [ 0.8540,  0.5715, -0.0237,  ...,  0.3108, -0.2230,  0.2037],\n",
       "        [ 0.3911,  0.4019, -0.1505,  ..., -0.0348,  0.0798,  0.5031]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"), lower_case_backup=True)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vectors.get_vecs_by_tokens(\"<BOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I missed the beginning but I did see most of it. A friend got it on DVD in the cheap room at FYE.<br /><br />The skits are all very short, and yet most of them are still too long. The majority of them, they seem to have forgotten to have something funny! Quite a lot of racist/sexist/\"homophobic\" humor in it, skits based on stereotypes, or skits which use racist terms for people.<br /><br />I'm trying to remember anything I thought was funny in it, and I'm having trouble.... The logo for the Tunnel Vision network is a lipsticked mouth with an eyeball in it. The mouth opens and closes over the eye like eyelids. Kind of creepy.<br /><br />What a disappointment. Most of the actors went on to better things, and it's lucky this bomb didn't hold them back.\n",
      "['I', 'missed', 'the', 'beginning', 'but', 'I', 'did', 'see', 'most', 'of', 'it', 'A', 'friend', 'got', 'it', 'on', 'DVD', 'in', 'the', 'cheap', 'room', 'at', 'FYE', ' ', 'The', 'skits', 'are', 'all', 'very', 'short', 'and', 'yet', 'most', 'of', 'them', 'are', 'still', 'too', 'long', 'The', 'majority', 'of', 'them', 'they', 'seem', 'to', 'have', 'forgotten', 'to', 'have', 'something', 'funny', 'Quite', 'a', 'lot', 'of', 'racistsexisthomophobic', 'humor', 'in', 'it', 'skits', 'based', 'on', 'stereotypes', 'or', 'skits', 'which', 'use', 'racist', 'terms', 'for', 'people', ' ', 'I', 'm', 'trying', 'to', 'remember', 'anything', 'I', 'thought', 'was', 'funny', 'in', 'it', 'and', 'I', 'm', 'having', 'trouble', 'The', 'logo', 'for', 'the', 'Tunnel', 'Vision', 'network', 'is', 'a', 'lipsticked', 'mouth', 'with', 'an', 'eyeball', 'in', 'it', 'The', 'mouth', 'opens', 'and', 'closes', 'over', 'the', 'eye', 'like', 'eyelids', 'Kind', 'of', 'creepy', ' ', 'What', 'a', 'disappointment', 'Most', 'of', 'the', 'actors', 'went', 'on', 'to', 'better', 'things', 'and', 'its', 'lucky', 'this', 'bomb', 'did', 'nt', 'hold', 'them', 'back']\n",
      "['miss', 'begin', 'see', 'friend', 'got', 'dvd', 'cheap', 'room', 'fye', 'skit', 'veri', 'short', 'yet', 'still', 'long', 'major', 'seem', 'forgotten', 'someth', 'funni', 'quit', 'lot', 'racistsexisthomophob', 'humor', 'skit', 'base', 'stereotyp', 'skit', 'use', 'racist', 'term', 'peopl', 'tri', 'rememb', 'anyth', 'thought', 'wa', 'funni', 'troubl', 'logo', 'tunnel', 'vision', 'network', 'lipstick', 'mouth', 'eyebal', 'mouth', 'open', 'close', 'eye', 'like', 'eyelid', 'kind', 'creepi', 'disappoint', 'actor', 'went', 'better', 'thing', 'lucki', 'bomb', 'nt', 'hold', 'back']\n",
      "['miss', 'begin', 'see', 'friend', 'got', 'dvd', 'cheap', 'room', 'fye', 'skit', 'veri', 'short', 'yet', 'still', 'long', 'major', 'seem', 'forgotten', 'someth', 'funni', 'quit', 'lot', 'racistsexisthomophob', 'humor', 'skit', 'base', 'stereotyp', 'skit', 'use', 'racist', 'term', 'peopl', 'tri', 'rememb', 'anyth', 'thought', 'wa', 'funni', 'troubl', 'logo', 'tunnel', 'vision', 'network', 'lipstick', 'mouth', 'eyebal', 'mouth', 'open', 'close', 'eye', 'like', 'eyelid', 'kind', 'creepi', 'disappoint', 'actor', 'went', 'better', 'thing', 'lucki', 'bomb', 'nt', 'hold', 'back']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/lixiaochuan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# preprocess text \n",
    "# 1.lemmonize\n",
    "# 2.remove stop words \n",
    "# 3.remove punctuations \n",
    "\n",
    "import spacy\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "import string\n",
    "import re\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# tokenizer = spacy.load(\"en_core_web_sm\")\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "\n",
    "def remove_punctuation(s):\n",
    "    # need to remove \"br\"\n",
    "    # how to split racist/sexist/\"homophobic\" ? \n",
    "    s = re.sub(r'<br />', ' ', s)\n",
    "    s = s.translate(str.maketrans('', '', string.punctuation))\n",
    "    s = re.sub(r'\\n', ' ', s)    \n",
    "    return s \n",
    " \n",
    "def stem_words(words):\n",
    "    stemmed_words = [stemmer.stem(word) for word in words]\n",
    "    return stemmed_words\n",
    "\n",
    "def lemmatize(words):\n",
    "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
    "    return lemmatized_words\n",
    "\n",
    "def remove_stopwords(words):\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    stopwords.append(\" \")\n",
    "    words = [word for word in words if word.lower() not in stopwords]\n",
    "    return words\n",
    "\n",
    "print(train_data[data_index][1])\n",
    "\n",
    "words = tokenizer(remove_punctuation(train_data[data_index][1]))\n",
    "# words = [token for token in doc] # no need\n",
    "print(words)\n",
    "words = remove_stopwords(stem_words(lemmatize(words)))\n",
    "print(words)\n",
    "\n",
    "def pre_process(text):\n",
    "    words = tokenizer(remove_punctuation(text))\n",
    "    # words = [token for token in doc] # no need\n",
    "    # print(words)\n",
    "    words = remove_stopwords(stem_words(lemmatize(words)))\n",
    "    # print(words)\n",
    "    return words\n",
    "\n",
    "print(pre_process(train_data[data_index][1]))\n",
    "#print(stem_words(words))# t1[:40],\n",
    "#print(lemmatize(stem_words(words)))\n",
    "#print(remove_stopwords(lemmatize(stem_words(words))))\n",
    "\n",
    "# exiting issues \n",
    "# 1. how to keep wasn't\n",
    "# 2. how to split racist/sexist/\"homophobic\" ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239778,
     "status": "ok",
     "timestamp": 1615842157475,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "Sc9_0aaBODCh",
    "outputId": "85e18e96-9ec4-42fa-fe04-478d5e10a886"
   },
   "outputs": [],
   "source": [
    "# this is using the frequencies in the data \n",
    "\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch#Load-Glove-'42B'-Embeddings\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from itertools import chain\n",
    "\n",
    "# tokenizer(next(train_iter)[1])\n",
    "\n",
    "\n",
    "# create vocabulatory   \n",
    "counter = Counter()\n",
    "for (label, line) in chain(train_iter,valid_data):\n",
    "    counter.update(pre_process(line))\n",
    "\n",
    "# be careful, lower case vocab to call function   \n",
    "vocab = vocab(counter, min_freq=10, specials=('<unk>', '<BOS>', '<EOS>', '<PAD>'))\n",
    "vocab.set_default_index(vocab['<unk>'])  # default index for oov words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16620, 'enter', 0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab.lookup_token(21), vocab['were']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set up embedding for the vocabulatory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:1')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embedding for dat\n",
    "\n",
    "# embed_len = 100\n",
    "\n",
    "glove_embedding_tensor = torch.zeros(len(vocab),  embed_len).to(device)\n",
    " \n",
    "glove_embedding_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(vocab)):\n",
    "    try: \n",
    "        glove_embedding_tensor[i] = global_vectors.get_vecs_by_tokens(vocab.lookup_token(i))\n",
    "    #except KeyError as e:\n",
    "    #    glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "    except KeyError:\n",
    "        glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([16620, 200]),\n",
       " tensor([[-0.0492, -0.1199,  0.0567,  0.4816,  0.5375, -0.4658, -0.3448,  0.0903,\n",
       "          -0.1198,  0.3473, -0.3397,  0.1079,  0.1423, -0.0274,  0.1822,  0.1699,\n",
       "           0.4808,  0.5828, -0.1633,  0.6667,  0.2206,  2.4306, -0.6263,  0.0826,\n",
       "          -0.3050, -0.0593,  0.0486,  0.4304, -0.0060,  0.0616,  0.4506, -0.1367,\n",
       "          -0.4818,  0.3655,  0.0679,  0.2511, -0.0102, -0.4010, -0.2609,  0.0096,\n",
       "           0.3771, -0.0950, -0.5507,  0.2726, -0.1458, -0.1442,  0.1495,  0.0570,\n",
       "          -0.2200, -0.1429, -0.4358, -0.0153, -0.4280, -0.0278,  0.4599, -0.2551,\n",
       "           0.4442,  0.2590, -0.1597,  0.1803,  0.4232,  0.4250, -0.2827,  0.3910,\n",
       "           0.1220,  0.5496,  0.1232,  0.8240,  0.0724,  0.3928,  0.5629,  0.4256,\n",
       "           0.3198,  0.2332, -0.2711,  0.2348,  0.0541, -0.2120,  0.2543, -0.2494,\n",
       "          -0.0307,  0.2610,  0.2042,  0.2186, -0.0366,  0.2053, -0.4181, -0.6570,\n",
       "           1.3602, -0.5905,  0.1831,  0.0769, -0.1622,  0.0415, -0.2920,  0.0129,\n",
       "           0.3157,  0.1596, -0.1207, -0.3274,  0.2254,  0.2553, -0.5223, -0.1198,\n",
       "           0.0331, -0.0322,  0.2446,  1.1988, -0.1000, -0.3010, -0.3636,  0.1643,\n",
       "          -0.4521, -0.0341, -0.2811, -0.2118, -0.1160, -0.4503,  0.3775,  0.0324,\n",
       "           0.1216, -0.5751,  0.1383,  0.0523, -0.4724, -0.0808,  0.1615, -0.1766,\n",
       "          -0.1970, -0.2246,  0.0399,  0.2779, -0.1069,  0.2392, -0.0517, -0.0177,\n",
       "          -0.2836,  0.0986,  0.5660, -0.3067,  0.2091, -0.0605, -0.4316, -0.3409,\n",
       "           0.8001,  0.3284, -0.4829, -0.1518, -0.5951,  0.1740, -0.5164, -0.0293,\n",
       "           0.1595,  0.0445,  0.0641,  0.0390,  0.2574,  0.2640,  0.3741, -0.0716,\n",
       "          -0.3008,  0.5298,  0.5515, -1.2123, -0.2237, -0.0727,  0.4552, -0.2741,\n",
       "           0.0938,  0.6188, -0.2261,  0.1065,  0.2466,  0.6439,  0.4569,  0.0257,\n",
       "          -0.2643,  0.0932,  0.0609,  0.5640,  0.9659,  0.5004,  0.3856, -0.2672,\n",
       "          -0.0988,  0.5960,  0.0366, -0.0033,  0.1319,  0.3690, -0.4550,  0.1619,\n",
       "          -0.4011,  0.5456,  0.0794, -0.1256, -0.4847,  0.0951, -0.0430,  0.1508],\n",
       "         [-0.0233, -0.5157,  0.3300, -0.8471, -0.1271, -0.0292, -0.1597, -0.3164,\n",
       "           0.2863,  0.2097,  0.1728,  0.1588, -0.4438,  0.4538,  0.1488,  0.3493,\n",
       "           0.5207,  0.6321,  0.0903,  0.0409,  0.2466,  2.0028, -0.1500,  0.3357,\n",
       "          -0.5376, -0.4959, -0.3219, -0.3044,  0.5254,  0.1333, -0.7969, -0.3987,\n",
       "          -0.2429,  1.0002, -0.1085,  0.1539, -0.7234, -0.0026, -0.0630,  0.7370,\n",
       "           0.6318, -0.2625,  0.0972,  0.0207,  0.5742, -0.1742, -0.4175, -0.0074,\n",
       "           0.8375,  0.4959, -0.7977, -0.0079, -0.3593, -0.0565,  0.8170, -0.0693,\n",
       "           0.3259, -0.4015,  0.3184,  0.1724, -0.7075,  0.4188,  0.0352,  0.5860,\n",
       "           0.9225, -0.3123,  0.5263,  0.2474, -0.6897, -0.1427, -0.4696,  0.0240,\n",
       "          -0.6213,  0.3033, -0.2547, -0.2285, -0.2221, -0.0837,  1.1281, -0.3934,\n",
       "          -0.6730,  0.2279,  0.0123, -0.2296, -0.8895, -0.3116,  0.2362, -0.0649,\n",
       "           0.4162, -0.0328,  0.7211,  0.0293,  0.3980, -0.0746, -0.5849,  0.1775,\n",
       "          -0.2741,  0.1820, -0.0616,  0.3065,  0.3500, -0.1515, -0.2373,  0.4300,\n",
       "          -0.5618, -0.0565,  0.1682,  0.1299,  0.7462, -0.2727,  0.6788, -0.3918,\n",
       "          -0.5537,  0.7273, -0.2215,  0.4371, -0.0527,  0.1165, -0.0734,  0.0779,\n",
       "          -0.1485,  0.5850, -0.1569, -0.2045,  0.5570,  0.1474,  0.1966, -0.1780,\n",
       "          -0.2530, -0.0920, -0.2434,  0.4668,  0.0529, -0.0228, -0.4077,  0.3737,\n",
       "           0.1651, -0.8634, -0.2161,  0.0702,  0.2487, -0.4277, -0.1069, -0.0830,\n",
       "           0.5633, -0.0308,  0.3919, -0.7734,  0.3016,  0.1828, -0.0691,  0.6696,\n",
       "          -0.7196, -0.0244, -0.0626, -0.3007,  0.0794,  0.5789,  0.1563, -0.3352,\n",
       "          -0.7457,  0.0899,  0.3782,  0.3214, -0.1934,  0.1470,  0.1283,  0.6407,\n",
       "          -0.0051,  0.5034,  0.3865,  0.0930, -0.1429,  0.0144, -0.4918, -0.5997,\n",
       "          -0.0208,  0.2154,  0.1673,  0.2336,  0.6237,  0.2167,  0.2867, -0.4246,\n",
       "          -0.2231, -0.1193,  0.2633, -0.0024, -0.3507, -0.2413,  0.1741, -0.3333,\n",
       "          -0.5321,  0.2567,  0.1217,  0.1650,  0.6093, -0.0913, -0.6544,  0.0648]],\n",
       "        device='cuda:1'))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_transform = lambda x: [global_vectors.get_vecs_by_tokens('<BOS>')] + [global_vectors.get_vecs_by_tokens(token) for token in tokenizer(x)] + [global_vectors.get_vecs_by_tokens('<EOS>')]\n",
    "\n",
    "#t1 = text_transform(\"how are you?\")\n",
    " \n",
    "glove_embedding_tensor.shape,glove_embedding_tensor[1232:1234,]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(x)] + [vocab['<EOS>']]\n",
    "def text_transform(x):\n",
    "     words = tokenizer(remove_punctuation(x))\n",
    "\n",
    "     #words = [token.text for token in doc]\n",
    "     # print(words)\n",
    "     words = remove_stopwords(stem_words(lemmatize(words)))\n",
    "     # print(words)\n",
    "     words =  [vocab['<BOS>']] + [vocab[word] for word in words] + [vocab['<EOS>']]\n",
    "\n",
    "     return words\n",
    "\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "    \n",
    "def collate_batch(batch):\n",
    "   label_list, text_list, sent_len = [], [], []\n",
    "   for (_label, _text) in batch: # each represents one text\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "        sent_len.append(len(text_transform(_text)))\n",
    "   \n",
    "   sent_len = torch.tensor(sent_len, dtype=torch.long)\n",
    "   sent_len, perm_idx = sent_len.sort(0, descending=True)\n",
    "   text_list  = [text_list[i] for i in perm_idx]\n",
    "   label_list = [label_list[i] for i in perm_idx]\n",
    "   return torch.tensor(label_list, dtype=torch.long).to(device), pad_sequence(text_list,  padding_value=1.0).to(device), sent_len.to(device) # not using batch_first=True,\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "EMy__zqIPbll"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True , \n",
    "                              collate_fn=collate_batch)\n",
    " \n",
    "                  # collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_data, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "EWQrfaT46CxW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[   1,    1,    1,  ...,    1,    1,    1],\n",
       "         [ 121, 1097,   24,  ..., 4266,  686,  160],\n",
       "         [ 657,   40,  336,  ...,  160,    4,  121],\n",
       "         ...,\n",
       "         [   0,    1,    1,  ...,    1,    1,    1],\n",
       "         [4237,    1,    1,  ...,    1,    1,    1],\n",
       "         [   2,    1,    1,  ...,    1,    1,    1]], device='cuda:1'),\n",
       " tensor([0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1,\n",
       "         1, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0,\n",
       "         1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0], device='cuda:1'),\n",
       " 499,\n",
       " 64,\n",
       " torch.Size([499, 64]),\n",
       " tensor([499, 350, 350, 294, 270, 258, 238, 234, 232, 232, 196, 194, 191, 150,\n",
       "         146, 143, 137, 126, 121, 120, 114, 110, 108, 105, 105, 104, 103,  99,\n",
       "          99,  97,  97,  96,  93,  93,  90,  90,  89,  87,  85,  83,  80,  78,\n",
       "          77,  76,  76,  74,  72,  71,  70,  68,  67,  67,  63,  62,  61,  52,\n",
       "          49,  42,  39,  38,  31,  29,  29,  28], device='cuda:1'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel, trainfeature,sent_len = next(iter(train_dataloader)) \n",
    "trainfeature,trainlabel, len(trainfeature), len(trainlabel), trainfeature.shape, sent_len  # len is by row\n",
    "\n",
    "\n",
    "# 1182 is the longest length of the batch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnqlCW9arhLP"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YcA1qdwD6CxY"
   },
   "outputs": [],
   "source": [
    "# referene : https://coderzcolumn.com/tutorials/artificial-intelligence/pytorch-rnn-for-text-classification-tasks#2\n",
    " \n",
    " # simple RNN ,model\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "class RNNClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_len, hidden_dim, output_dim, n_layers, pad_idx):\n",
    "        super(RNNClassifier, self).__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_len, padding_idx = pad_idx)\n",
    "        \n",
    "        \n",
    "        self.rnn = nn.RNN(input_size=embed_len, \n",
    "                       hidden_size=hidden_dim, \n",
    "                       num_layers=n_layers, \n",
    "                       batch_first=True)            \n",
    "\n",
    " \n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "         \n",
    "\n",
    "    def forward(self, text, sent_len):\n",
    "        embedded = self.embedding(text)\n",
    "\n",
    "        embedded = embedded.permute(1, 0, 2)# need to put the batch first \n",
    "        \n",
    "        packed_input = pack_padded_sequence(embedded, sent_len.cpu().numpy(), batch_first=True)\n",
    "        # packed_input.batch_sizes.cpu().numpy().sum()\n",
    "        # packed_input.data.shape\n",
    "        packed_output, ht = self.rnn(packed_input)\n",
    "\n",
    "        output, input_sizes = pad_packed_sequence(packed_output, batch_first=True)\n",
    "\n",
    "        return self.fc(ht[-1]) # predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "w43pDDgT6CxY"
   },
   "outputs": [],
   "source": [
    "vocab_size = len(vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "hidden_dim = 50\n",
    "OUTPUT_DIM = 1\n",
    "# DROPOUT = 0.5\n",
    "\n",
    "n_layers=1\n",
    "PAD_IDX = vocab.lookup_indices([\"<PAD>\"])[0]\n",
    "\n",
    "rnn_model = RNNClassifier(vocab_size, EMBEDDING_DIM, hidden_dim, OUTPUT_DIM, n_layers, PAD_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRrHD3Ss6Cxb"
   },
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [-0.0088, -0.5700,  0.1249,  ..., -0.5161,  0.2882, -0.6986],\n",
       "        [ 0.0399,  0.2983,  0.2843,  ...,  0.1468,  0.0138, -0.4792],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rnn_model.embedding = torch.nn.Embedding.from_pretrained(glove_embedding_tensor,freeze=False)\n",
    "rnn_model.embedding.weight.data.copy_(glove_embedding_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "7lpQKr4u-qjA"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(rnn_model.parameters())\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "rnn_model = rnn_model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211542,
     "status": "ok",
     "timestamp": 1615844247885,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "B7hBXR41-4HE",
    "outputId": "8dbc4e39-8490-499d-fa90-654d81f60e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train: Loss: 0.4742 Acc: 0.7774\n",
      "Epoch 0 Valid: Loss: 0.4867 Acc: 0.7968\n",
      "Epoch 1 Train: Loss: 0.4932 Acc: 0.7638\n",
      "Epoch 1 Valid: Loss: 0.5382 Acc: 0.7506\n",
      "Epoch 2 Train: Loss: 0.4133 Acc: 0.8127\n",
      "Epoch 2 Valid: Loss: 0.5598 Acc: 0.7296\n",
      "Epoch 3 Train: Loss: 0.3381 Acc: 0.8570\n",
      "Epoch 3 Valid: Loss: 0.5187 Acc: 0.7713\n",
      "Epoch 4 Train: Loss: 0.2754 Acc: 0.8909\n",
      "Epoch 4 Valid: Loss: 0.5480 Acc: 0.7716\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  \n",
    "  rnn_model.train()\n",
    "  for label, text, sent_len in train_dataloader:\n",
    "      #print(label.dtype)\n",
    "      #print(text)\n",
    "      optimizer.zero_grad()\n",
    "      predictions = rnn_model(text, sent_len).squeeze(1)\n",
    "      #print(predictions.dtype)\n",
    "      loss = criterion(predictions, label.float())\n",
    "      \n",
    "      rounded_preds = torch.round(\n",
    "          torch.sigmoid(predictions))\n",
    "      correct = \\\n",
    "        (rounded_preds == label).float()\n",
    "      acc = correct.sum() / len(correct)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(train_dataloader), \n",
    "          epoch_acc / len(train_dataloader)))\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  rnn_model.eval()\n",
    "  # with torch.no_grad():\n",
    "  #   for label, text  , sent_len in valid_dataloader:\n",
    "  #     predictions = rnn_model(text, sent_len).squeeze(1)\n",
    "  #     loss = criterion(predictions, label.float())\n",
    "      \n",
    "  #     rounded_preds = torch.round(\n",
    "  #         torch.sigmoid(predictions))\n",
    "  #     correct = \\\n",
    "  #       (rounded_preds == label).float()\n",
    "  #     acc = correct.sum() / len(correct)\n",
    "      \n",
    "  #     epoch_loss += loss.item()\n",
    "  #     epoch_acc += acc.item()\n",
    "\n",
    "  with torch.no_grad():\n",
    "      for label, text , sent_len in valid_dataloader:\n",
    "          predictions = rnn_model(text, sent_len).squeeze(1)\n",
    "          loss = criterion(predictions, label.float())\n",
    "          \n",
    "          rounded_preds = torch.round(\n",
    "              torch.sigmoid(predictions))\n",
    "          correct = \\\n",
    "            (rounded_preds == label).float()\n",
    "          acc = correct.sum() / len(correct)\n",
    "          \n",
    "          epoch_loss += loss.item()\n",
    "          epoch_acc += acc.item()\n",
    "\n",
    "\n",
    "  print(\"Epoch %d Valid: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(valid_dataloader), \n",
    "          epoch_acc / len(valid_dataloader)))\n",
    "  \n",
    "# out: (your results may vary)\n",
    "# Epoch 0 Train: Loss: 0.6523 Acc: 0.7165\n",
    "# Epoch 0 Valid: Loss: 0.5259 Acc: 0.7474\n",
    "# Epoch 1 Train: Loss: 0.5935 Acc: 0.7765\n",
    "# Epoch 1 Valid: Loss: 0.4571 Acc: 0.7933\n",
    "# Epoch 2 Train: Loss: 0.5230 Acc: 0.8257\n",
    "# Epoch 2 Valid: Loss: 0.4103 Acc: 0.8245\n",
    "# Epoch 3 Train: Loss: 0.4559 Acc: 0.8598\n",
    "# Epoch 3 Valid: Loss: 0.3828 Acc: 0.8549\n",
    "# Epoch 4 Train: Loss: 0.4004 Acc: 0.8813\n",
    "# Epoch 4 Valid: Loss: 0.3781 Acc: 0.8675"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfBcC4qWDkcS"
   },
   "source": [
    "# Testing & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41614,
     "status": "ok",
     "timestamp": 1615844337404,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "WdDAlQlaDP0-",
    "outputId": "5a523532-a9b5-4cc4-8803-84c9ad121358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss: 0.5590 Acc: 0.7672\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_acc = 0\n",
    "rnn_model.eval() # <1>\n",
    "with torch.no_grad(): # <1>\n",
    "  for label, text, sent_len in test_dataloader:\n",
    "    predictions = rnn_model(text, sent_len).squeeze(1)\n",
    "    loss = criterion(predictions, label.float())\n",
    "    \n",
    "    rounded_preds = torch.round(\n",
    "        torch.sigmoid(predictions))\n",
    "    correct = \\\n",
    "      (rounded_preds == label).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    test_acc += acc.item()\n",
    "\n",
    "print(\"Test: Loss: %.4f Acc: %.4f\" %\n",
    "        (test_loss / len(test_dataloader), \n",
    "        test_acc / len(test_dataloader)))\n",
    "# out: (your results will vary)\n",
    "#   Test: Loss: 0.3821 Acc: 0.8599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1615844842113,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "yncFrpAcD2GY",
    "outputId": "fbd7ff12-c2ce-410c-e8d2-88dfdbb80487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23295678198337555\n",
      "0.8881576061248779\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] \n",
    "      for token in pre_process(x)] # significantly change results\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    sent_encoded = text_pipeline(sentence) \n",
    "    text = torch.tensor(sent_encoded).unsqueeze(1).to(device)\n",
    "    prediction = torch.sigmoid(model(text, torch.tensor([len(sent_encoded)])).to(device))\n",
    "    return prediction.item()\n",
    "\n",
    "sentiment = predict_sentiment(rnn_model, \n",
    "                  \"Don't waste your time\")\n",
    "print(sentiment)\n",
    "# out: 4.763594888613835e-34\n",
    "\n",
    "sentiment = predict_sentiment(rnn_model, \n",
    "                  \"You gotta see this movie!\")\n",
    "print(sentiment)\n",
    "# out: 0.941755473613739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adVgHt83EGbs"
   },
   "outputs": [],
   "source": [
    "\n",
    "torch.save(rnn_model.state_dict(), 'basic_rnn-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pJ2CmUZq21-6"
   },
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1Y66iHpZIpvg6TX6U00ZP",
   "collapsed_sections": [],
   "name": "04_02_Sentiment_Analysis_with_TorchText.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8d1b3b63f0ab60c7236a713968b959123020c0d6fb9cdd1ffb50b409abe40ad8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
