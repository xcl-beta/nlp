{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y2fkP_np3Zpl"
   },
   "source": [
    "<!--BOOK_INFORMATION-->\n",
    "<img align=\"left\" style=\"width:80px;height:98px;padding-right:20px;\" src=\"https://raw.githubusercontent.com/joe-papa/pytorch-book/main/files/pytorch-book-cover.jpg\">\n",
    "\n",
    "This notebook contains an excerpt from the [PyTorch Pocket Reference](http://pytorchbook.com) book by [Joe Papa](http://joepapa.ai); content is available [on GitHub](https://github.com/joe-papa/pytorch-book)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlA2i0FpGPxe"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/joe-papa/pytorch-book/blob/main/04_02_Sentiment_Analysis_with_TorchText.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aXkszegKMMMn"
   },
   "source": [
    "# Chapter 4 - Neural Network Development Reference Designs\n",
    "# Sentiment Analysis with TorchText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ5SE1D2mFMs"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PAG915JevD3i"
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random_state = random.seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if \n",
    "    torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm') # <1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1615841842114,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "4u0I1d526CxQ",
    "outputId": "7b3d42b5-37a0-436d-df45-c1d4eda3c973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'movie', 'is', 'awesome', 'This movie', 'is awesome', 'movie is']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "\n",
    "generate_bigrams([\n",
    " 'This', 'movie', 'is', 'awesome'])\n",
    "# out:\n",
    "# ['This', 'movie', 'is', 'awesome', 'This movie',\n",
    "#  'movie is', 'is awesome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "LLCXyoKT6CxU"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchtext import data\n",
    "# from torchtext import datasets\n",
    "\n",
    "# TEXT = data.Field(tokenize = 'spacy',\n",
    "#                   preprocessing = \\\n",
    "#                     generate_bigrams) # <1>\n",
    "\n",
    "# LABEL = data.LabelField(dtype = torch.float) # <2>\n",
    "\n",
    "# train_data, test_data = \\\n",
    "#   datasets.IMDB.splits(TEXT, LABEL) # <3>\n",
    "# train_data, valid_data = train_data.split(\n",
    "#     random_state=random_state) # <4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "AEjFcr_aJgIr"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_iter, test_iter = IMDB(\n",
    "    split=('train', 'test')) #<1>\n",
    "\n",
    "train_dataset = list(train_iter) #<2>\n",
    "test_dataset  = list(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.70)\n",
    "train_data, valid_data = \\\n",
    "    random_split(train_dataset, \n",
    "        [num_train, \n",
    "         len(train_dataset) - num_train]) # <3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1615841915453,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "7IOq1ufhq01Q",
    "outputId": "ffa9f08a-66ac-4119-a257-c4fd2c035ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 7500 25000\n",
      "neg\n",
      "I missed the beginning but I did see most of it. A friend got it on DVD in the cheap room at FYE.<br /><br />The skits are all very short, and yet most of them are still too long. The majority of them, they seem to have forgotten to have something funny! Quite a lot of racist/sexist/\"homophobic\" humor in it, skits based on stereotypes, or skits which use racist terms for people.<br /><br />I'm trying to remember anything I thought was funny in it, and I'm having trouble.... The logo for the Tunnel Vision network is a lipsticked mouth with an eyeball in it. The mouth opens and closes over the eye like eyelids. Kind of creepy.<br /><br />What a disappointment. Most of the actors went on to better things, and it's lucky this bomb didn't hold them back.\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data), len(test_dataset))\n",
    "# out:17500 7500 25000\n",
    "\n",
    "data_index = 21\n",
    "print(train_data[data_index][0])\n",
    "# out: (your results may vary)\n",
    "#   pos\n",
    "\n",
    "print(train_data[data_index][1])\n",
    "# out: (your results may vary)\n",
    "# ['This', 'film', 'moved', 'me', 'beyond', 'comprehension', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embed_len = 200\n",
    "global_vectors = GloVe(name='6B', dim=embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2661,  0.2182, -0.1100,  ..., -0.1198, -0.1916, -0.1352],\n",
       "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
       "        [ 0.1815,  0.2663,  0.0550,  ...,  0.5375,  0.3151,  0.0162],\n",
       "        [ 0.0367,  0.1989, -0.0930,  ..., -0.0133, -0.0039,  0.7128],\n",
       "        [ 0.8540,  0.5715, -0.0237,  ...,  0.3108, -0.2230,  0.2037],\n",
       "        [ 0.3911,  0.4019, -0.1505,  ..., -0.0348,  0.0798,  0.5031]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"), lower_case_backup=True)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vectors.get_vecs_by_tokens(\"<BOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239778,
     "status": "ok",
     "timestamp": 1615842157475,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "Sc9_0aaBODCh",
    "outputId": "85e18e96-9ec4-42fa-fe04-478d5e10a886"
   },
   "outputs": [],
   "source": [
    "# this is using the frequencies in the data \n",
    "\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch#Load-Glove-'42B'-Embeddings\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "# tokenizer(next(train_iter)[1])\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<BOS>', '<EOS>', '<PAD>'])\n",
    "        \n",
    "# # create vocabulatory\n",
    "# counter = Counter()\n",
    "# for (label, line) in train_iter:\n",
    "#     counter.update(tokenizer(line))\n",
    "\n",
    "# # be careful, lower case vocab to call function   \n",
    "# vocab = vocab(counter, min_freq=10, specials=('<unk>', '<BOS>', '<EOS>', '<PAD>'))\n",
    "vocab.set_default_index(vocab['<unk>'])  # default index for oov words\n",
    "\n",
    "# why use vocab?\n",
    "\n",
    "# change data from str to numeric \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121069, [164, 11, 45, 493], 'was', 21)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab(['here', 'is', 'an', 'example']), vocab.lookup_token(21), vocab['was']\n",
    "\n",
    "# THE LENGTH IS MUCH LARGER THAN USING COUNTER, WHICH IS 23404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embedding for dat\n",
    "\n",
    "# embed_len = 100\n",
    "\n",
    "glove_embedding_tensor = torch.zeros(len(vocab),  embed_len).to(device)\n",
    " \n",
    "glove_embedding_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(vocab)):\n",
    "    try: \n",
    "        glove_embedding_tensor[i] = global_vectors.get_vecs_by_tokens(vocab.lookup_token(i))\n",
    "    #except KeyError as e:\n",
    "    #    glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "    except KeyError:\n",
    "        glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:1')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_tensor[len(vocab)-2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'₤100'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(len(vocab)-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([121069, 200]),\n",
       " tensor([[-3.7880e-01,  5.1459e-01, -7.3694e-01, -5.6198e-02,  5.2874e-02,\n",
       "           1.5662e-01,  2.1436e-01,  3.4249e-01,  1.4240e-01, -9.3278e-02,\n",
       "           1.6948e-01, -1.1071e-01, -5.6108e-01, -6.0902e-01,  8.6495e-01,\n",
       "           8.8216e-02,  1.9156e-02, -1.8672e-01, -4.5746e-01, -2.9990e-01,\n",
       "           7.6822e-01,  1.9607e+00,  2.0502e-01, -2.4833e-01,  7.9986e-02,\n",
       "           2.2780e-01,  4.6934e-01, -9.3518e-02,  2.9816e-01,  3.3182e-02,\n",
       "           3.8590e-01,  4.7495e-02, -1.7896e-01, -7.4432e-01, -6.6266e-01,\n",
       "          -4.5246e-02, -4.2622e-01, -4.5248e-01,  7.1457e-02, -4.4291e-01,\n",
       "          -4.4779e-01,  4.8310e-03, -7.2398e-01, -1.5956e-01, -3.1243e-01,\n",
       "          -1.8692e-01,  5.8064e-01,  4.1169e-01,  1.1543e-01,  4.9991e-01,\n",
       "           3.8175e-02, -4.4620e-01,  6.4993e-01,  1.1448e+00, -2.4783e-01,\n",
       "          -2.7646e-01, -4.7514e-01,  6.5408e-01, -6.5782e-02, -2.7297e-01,\n",
       "           2.4473e-03, -1.5304e-01, -5.6501e-01, -3.2696e-01,  1.3107e+00,\n",
       "           4.9093e-01, -2.8914e-01,  1.7231e-01,  7.6938e-01, -2.8518e-01,\n",
       "          -2.2873e-01, -5.0518e-01,  6.0937e-01,  1.4541e-01,  3.9290e-01,\n",
       "           5.5956e-02, -5.5703e-01, -3.8893e-02, -2.0495e-01,  5.6423e-01,\n",
       "           8.2132e-03, -6.6767e-02, -2.9927e-01,  6.0117e-01, -6.3384e-02,\n",
       "          -6.4189e-01, -5.5444e-01,  5.0138e-01, -3.6673e-01, -4.9304e-02,\n",
       "          -7.2545e-01, -2.9274e-01, -2.1346e-03,  1.8312e-01,  4.6072e-01,\n",
       "          -7.7038e-01,  2.6467e-01,  3.3044e-01,  2.2858e-01, -4.4268e-01,\n",
       "          -5.4123e-01,  3.8888e-01, -1.4331e-01,  1.1509e-01,  1.2356e-01,\n",
       "          -1.5888e-01,  1.5258e-01,  1.7362e+00, -3.9500e-01, -2.7246e-01,\n",
       "           2.0712e-02, -2.2041e-01, -5.7030e-01,  2.3306e-01, -1.0842e+00,\n",
       "           8.6591e-01, -5.1683e-01,  3.7414e-01,  2.0773e-01,  5.4710e-02,\n",
       "          -3.2593e-01,  5.6176e-01, -3.3873e-02,  5.9226e-01,  2.2097e-01,\n",
       "          -9.9521e-01, -7.0702e-01, -6.0332e-01,  7.5742e-01,  3.7470e-03,\n",
       "          -3.9539e-01, -4.1141e-02,  5.2373e-01, -1.0311e-01,  6.4206e-01,\n",
       "           6.3670e-01, -1.8812e-01,  1.3518e-01, -1.0743e+00,  9.8547e-02,\n",
       "          -3.2638e-02, -1.9948e-01, -3.2529e-01, -1.5256e-01,  6.6934e-01,\n",
       "           5.4685e-01,  2.2348e-01,  6.4717e-03,  7.9020e-02,  8.4018e-01,\n",
       "          -3.7394e-01,  4.0915e-01,  2.0513e-01, -3.7579e-01, -3.8788e-02,\n",
       "          -4.6400e-01, -2.6361e-01,  3.5504e-01, -5.2044e-01, -2.6436e-01,\n",
       "          -4.4091e-01,  2.1041e-01, -1.7172e-01,  5.2400e-02, -2.0947e-01,\n",
       "           4.8091e-02,  3.7248e-01,  2.8542e-01, -2.6875e-01,  2.7937e-01,\n",
       "          -1.2130e-02,  4.2894e-02,  1.9882e-01, -3.1488e-01,  1.0614e-01,\n",
       "           2.7269e-01, -1.0340e+00, -6.1990e-01, -4.6450e-02,  1.3484e-01,\n",
       "           1.2803e+00,  3.4117e-01,  1.5021e-01,  5.1793e-01,  1.0658e-01,\n",
       "          -4.5049e-01,  3.9682e-02,  6.2014e-01,  4.1656e-01, -3.7139e-01,\n",
       "          -2.1261e-01,  4.9009e-01, -2.9024e-01, -4.3822e-02,  3.6139e-01,\n",
       "           8.6226e-01,  4.8318e-01,  4.4144e-03,  1.8841e-01, -3.8341e-01],\n",
       "         [-6.5865e-01,  5.3423e-01,  4.3777e-02,  6.7393e-01,  1.6602e-01,\n",
       "          -4.1741e-01,  3.9818e-01,  5.3211e-01, -3.3676e-01,  7.9238e-01,\n",
       "          -3.1563e-01,  4.4786e-01,  2.5974e-01, -5.9847e-01,  2.0954e-01,\n",
       "           4.5330e-01,  5.9771e-01,  4.0166e-01, -3.2031e-01, -2.5832e-01,\n",
       "           7.7701e-01,  2.9981e+00, -3.5204e-01,  6.2900e-01,  2.8480e-01,\n",
       "          -2.4623e-02,  3.4439e-01, -1.4069e-01, -6.8782e-01,  6.2096e-01,\n",
       "          -5.1506e-01, -7.2010e-01,  3.0794e-01, -3.7843e-01, -4.9103e-01,\n",
       "           1.0848e+00,  4.4457e-01, -4.6116e-02,  1.4095e-01,  1.3767e-01,\n",
       "           2.5804e-02,  2.0366e-01, -2.8700e-02, -1.3788e-01,  2.2944e-02,\n",
       "          -3.2601e-01,  1.3869e-01,  9.5038e-01,  1.4023e-01,  1.3031e-01,\n",
       "          -6.2936e-01, -8.1791e-01,  4.2066e-01,  1.8227e-01, -3.8384e-01,\n",
       "           1.9960e-01,  8.0223e-01, -7.3926e-01,  3.0488e-01,  6.8268e-01,\n",
       "          -2.0069e-02, -1.4479e-01, -1.6923e-01,  1.7878e-01,  6.7149e-01,\n",
       "          -1.8140e-01, -1.0353e+00,  9.8209e-01,  3.5636e-03,  2.8201e-02,\n",
       "           1.4808e-01, -5.8151e-01,  3.3226e-01,  2.2428e-01,  2.0088e-01,\n",
       "           3.8332e-01, -5.3412e-01, -7.2728e-02,  1.5515e-01, -3.3184e-01,\n",
       "           4.3845e-01, -4.9682e-01,  1.2097e-01, -3.4529e-01,  2.9147e-01,\n",
       "          -5.1180e-01, -8.1401e-01,  5.8428e-01,  9.5096e-01,  1.3717e-01,\n",
       "          -1.2384e+00,  1.4919e-01, -4.8114e-02, -1.2080e-01, -1.3829e-01,\n",
       "           5.9211e-02, -8.6004e-01,  9.3794e-01,  1.6802e-01, -5.8304e-01,\n",
       "           8.1835e-01,  1.7855e-01,  3.9457e-01, -1.3119e-01, -3.9576e-01,\n",
       "          -8.1821e-01, -4.4240e-01,  1.0366e+00,  5.1156e-01, -1.5614e-03,\n",
       "           4.7977e-01, -2.0372e-01, -2.2248e-01,  2.7932e-01,  1.0755e-01,\n",
       "           1.7175e-01,  4.9290e-02, -2.1216e-01, -8.2777e-02, -3.7686e-01,\n",
       "           6.0366e-01,  6.1655e-01,  1.4334e-01, -3.5357e-01,  8.0505e-01,\n",
       "          -3.1358e-01,  5.4217e-01, -2.2839e-01, -8.1750e-02,  9.4913e-01,\n",
       "          -3.9255e-02, -3.4487e-01,  5.0564e-01, -4.9076e-01,  4.2899e-01,\n",
       "          -1.9273e-01, -5.1348e-01, -3.2148e-01, -2.9057e-02,  1.1805e-01,\n",
       "           2.1640e-01,  8.1328e-02,  5.9382e-01, -4.0867e-01,  1.7804e+00,\n",
       "           4.6344e-02,  3.2554e-01, -2.6692e-01,  2.4442e-01, -3.9477e-01,\n",
       "           4.8686e-02,  7.6914e-01,  8.0889e-01, -2.7523e-01,  4.0131e-01,\n",
       "          -6.2193e-01, -5.9757e-01,  6.3340e-01, -2.6412e-01, -2.8552e-01,\n",
       "          -5.5509e-01,  4.4657e-01, -7.0154e-01,  1.9491e-01, -4.0020e-01,\n",
       "           2.4529e-01, -1.6255e-01, -1.3351e-01, -5.2555e-01,  9.0975e-01,\n",
       "          -2.8669e-01,  1.0088e-01, -8.8846e-02, -4.1312e-01,  4.6584e-01,\n",
       "          -2.5755e-02, -1.3266e+00, -5.7612e-01, -2.3418e-01,  5.9119e-02,\n",
       "           7.7479e-01,  1.2262e-01,  6.6893e-01, -3.5013e-01, -5.5826e-01,\n",
       "          -3.4336e-01, -1.2027e-01, -2.1892e-01,  4.3901e-01, -5.5061e-01,\n",
       "           2.9896e-02,  1.2844e-01,  1.6361e-01,  2.2208e-01,  7.6550e-03,\n",
       "           8.5989e-01,  2.9140e-01, -1.8455e-01, -5.8256e-01,  4.7415e-01]],\n",
       "        device='cuda:1'))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_transform = lambda x: [global_vectors.get_vecs_by_tokens('<BOS>')] + [global_vectors.get_vecs_by_tokens(token) for token in tokenizer(x)] + [global_vectors.get_vecs_by_tokens('<EOS>')]\n",
    "\n",
    "#t1 = text_transform(\"how are you?\")\n",
    " \n",
    "glove_embedding_tensor.shape,glove_embedding_tensor[1232:1234,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(x)] + [vocab['<EOS>']]\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "    \n",
    "def collate_batch(batch):\n",
    "   label_list, text_list = [], []\n",
    "   for (_label, _text) in batch: # each represents one text\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "   return torch.tensor(label_list, dtype=torch.long).to(device), pad_sequence(text_list,  padding_value=1.0).to(device) # not using batch_first=True,\n",
    "\n",
    "max_words = 1000\n",
    "# embed_len = 100\n",
    "\n",
    "def vectorize_batch(batch):\n",
    "    Y, X = list(zip(*batch))  # here X is the list of input texts \n",
    "    X = [tokenizer(x) for x in X] # list of tokenzied text\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X] # make same length\n",
    "    X_tensor = torch.zeros(len(batch), max_words, embed_len) # batch input \n",
    "    for i, tokens in enumerate(X):\n",
    "        X_tensor[i] = global_vectors.get_vecs_by_tokens(tokens)\n",
    "    return X_tensor.reshape(len(batch), -1), torch.tensor(Y) - 1 ## Subtracted 1 from labels to bring in range [0,1,2,3] from [1,2,3,4]\n",
    "\n",
    "# use pretrained Glove embdedding \n",
    "def vectorize_batch2(batch):\n",
    "    Y, X = list(zip(*batch)) \n",
    "    X = [tokenizer(x) for x in X]\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X]\n",
    "    X_tensor = torch.zeros(len(batch), max_words, embed_len)\n",
    "    for i, tokens in enumerate(X):\n",
    "        X_tensor[i] = global_vectors.get_vecs_by_tokens(tokens)\n",
    "    label_list  = []\n",
    "    for i, _label in enumerate(Y):\n",
    "        label_list.append(label_transform(_label))\n",
    "    return torch.tensor(label_list, dtype=torch.long).to(device), X_tensor.reshape(len(batch), -1)## Subtracted 1 from labels to bring in range [0,1,2,3] from [1,2,3,4]\n",
    "\n",
    "# update data disctionary with glove dictionary \n",
    "def vectorize_batch3(batch):\n",
    "    Y, X = list(zip(*batch)) \n",
    "    X = [tokenizer(x) for x in X]\n",
    "    X = [tokens+[\"\"] * (max_words-len(tokens))  if len(tokens)<max_words else tokens[:max_words] for tokens in X]\n",
    "    X_tensor = torch.zeros(len(batch), max_words, embed_len)\n",
    "    for i, tokens in enumerate(X):\n",
    "        X_tensor[i] = global_vectors.get_vecs_by_tokens(tokens)\n",
    "    label_list  = []\n",
    "    for i, _label in enumerate(Y):\n",
    "        label_list.append(label_transform(_label))\n",
    "    return torch.tensor(label_list, dtype=torch.long).to(device), X_tensor.reshape(len(batch), -1)## Subtracted 1 from labels to bring in range [0,1,2,3] from [1,2,3,4]\n",
    "\n",
    "\n",
    "# train_data_loader = DataLoader(dataset=train_data, batch_size=32, shuffle=True , collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "EMy__zqIPbll"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# def collate_batch(batch):\n",
    "#    label_list, text_list = [], []\n",
    "#    for (_label, _text) in batch:\n",
    "#         label_list.append(label_pipeline(_label))\n",
    "#         processed_text = torch.tensor(text_pipeline(_text))\n",
    "#         text_list.append(processed_text)\n",
    "#    return (torch.tensor(label_list, dtype=torch.float64).to(device), \n",
    "#           pad_sequence(text_list, \n",
    "#                        padding_value=1.0).to(device))\n",
    "\n",
    "# batch_size = 64    \n",
    "# def batch_sampler():\n",
    "#     indices = [(i, len(tokenizer(s[1]))) for i, s in enumerate(train_dataset)]\n",
    "#     random.shuffle(indices)\n",
    "#     pooled_indices = []\n",
    "#     # create pool of indices with similar lengths \n",
    "#     for i in rvectorize_batch2dices for current batch\n",
    "#     for i in range(0, len(pooled_indices), batch_size):\n",
    "#         yield pooled_indices[i:i + batch_size]\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True , \n",
    "                              collate_fn=collate_batch)\n",
    " \n",
    "                  # collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_data, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "EWQrfaT46CxW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [   13, 12542,    13,  ...,    68,  2642,   114],\n",
       "         [   88, 13689,  1859,  ...,    11,     5,   116],\n",
       "         ...,\n",
       "         [    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:1'),\n",
       " tensor([0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "         0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1,\n",
       "         0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1], device='cuda:1'),\n",
       " 1042,\n",
       " 64,\n",
       " torch.Size([1042, 64]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel, trainfeature = next(iter(train_dataloader)) \n",
    "trainfeature,trainlabel, len(trainfeature), len(trainlabel), trainfeature.shape, # len is by row\n",
    "\n",
    "\n",
    "# 1182 is the longest length of the batch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnqlCW9arhLP"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "YcA1qdwD6CxY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len] \n",
    "        # cnn expects batch first, but text shape is reversed \n",
    "        # text.shape [934, 64], label : [64] \n",
    "        embedded  = self.embedding(text) \n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)# need to put the batch first \n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim] , [64,1,962,200]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3)) \n",
    "        # conv_0 will output tensor of shape [64, 100, 960, 1], 100 - output chanel from 100  filters\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "        # size :[64,300]\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)   # [64,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3], 121069)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices([\"<PAD>\"]), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = vocab.lookup_indices([\"<PAD>\"])[0]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "w43pDDgT6CxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 24,454,401 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.2642,  0.0694, -0.3210,  ..., -0.0317, -0.1033, -0.0825],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(glove_embedding_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121069, 200])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yndecIPP6Cxa"
   },
   "source": [
    "Not forgetting to zero the initial weights of our unknown and padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRrHD3Ss6Cxb"
   },
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "7lpQKr4u-qjA"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211542,
     "status": "ok",
     "timestamp": 1615844247885,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "B7hBXR41-4HE",
    "outputId": "8dbc4e39-8490-499d-fa90-654d81f60e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train: Loss: 0.0070 Acc: 0.9985\n",
      "Epoch 0 Valid: Loss: 0.4509 Acc: 0.8741\n",
      "Epoch 1 Train: Loss: 0.0036 Acc: 0.9998\n",
      "Epoch 1 Valid: Loss: 0.4472 Acc: 0.8780\n",
      "Epoch 2 Train: Loss: 0.0026 Acc: 0.9998\n",
      "Epoch 2 Valid: Loss: 0.4638 Acc: 0.8776\n",
      "Epoch 3 Train: Loss: 0.0018 Acc: 0.9999\n",
      "Epoch 3 Valid: Loss: 0.4828 Acc: 0.8769\n",
      "Epoch 4 Train: Loss: 0.0014 Acc: 0.9997\n",
      "Epoch 4 Valid: Loss: 0.4972 Acc: 0.8777\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  \n",
    "  model.train()\n",
    "  for label, text in train_dataloader:\n",
    "      #print(label.dtype)\n",
    "      #print(text)\n",
    "      optimizer.zero_grad()\n",
    "      predictions = model(text).squeeze(1)\n",
    "      #print(predictions.dtype)\n",
    "      loss = criterion(predictions, label.float())\n",
    "      \n",
    "      rounded_preds = torch.round(\n",
    "          torch.sigmoid(predictions))\n",
    "      correct = \\\n",
    "        (rounded_preds == label).float()\n",
    "      acc = correct.sum() / len(correct)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(train_dataloader), \n",
    "          epoch_acc / len(train_dataloader)))\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for label, text in valid_dataloader:\n",
    "      predictions = model(text).squeeze(1)\n",
    "      loss = criterion(predictions, label.float())\n",
    "      \n",
    "      rounded_preds = torch.round(\n",
    "          torch.sigmoid(predictions))\n",
    "      correct = \\\n",
    "        (rounded_preds == label).float()\n",
    "      acc = correct.sum() / len(correct)\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  print(\"Epoch %d Valid: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(valid_dataloader), \n",
    "          epoch_acc / len(valid_dataloader)))\n",
    "  \n",
    "# out: (your results may vary)\n",
    "# Epoch 0 Train: Loss: 0.6523 Acc: 0.7165\n",
    "# Epoch 0 Valid: Loss: 0.5259 Acc: 0.7474\n",
    "# Epoch 1 Train: Loss: 0.5935 Acc: 0.7765\n",
    "# Epoch 1 Valid: Loss: 0.4571 Acc: 0.7933\n",
    "# Epoch 2 Train: Loss: 0.5230 Acc: 0.8257\n",
    "# Epoch 2 Valid: Loss: 0.4103 Acc: 0.8245\n",
    "# Epoch 3 Train: Loss: 0.4559 Acc: 0.8598\n",
    "# Epoch 3 Valid: Loss: 0.3828 Acc: 0.8549\n",
    "# Epoch 4 Train: Loss: 0.4004 Acc: 0.8813\n",
    "# Epoch 4 Valid: Loss: 0.3781 Acc: 0.8675\n",
    "\n",
    "\n",
    "# WHY THE RESULT IS VERY CLOSE TO CREATING VOCAB USING COUNTER\n",
    "# ALTHOUGH THE SIZE OF VOCAB IS VEYR DIFFERENT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfBcC4qWDkcS"
   },
   "source": [
    "# Testing & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41614,
     "status": "ok",
     "timestamp": 1615844337404,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "WdDAlQlaDP0-",
    "outputId": "5a523532-a9b5-4cc4-8803-84c9ad121358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss: 0.5072 Acc: 0.8701\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_acc = 0\n",
    "model.eval() # <1>\n",
    "with torch.no_grad(): # <1>\n",
    "  for label, text in test_dataloader:\n",
    "    predictions = model(text).squeeze(1)\n",
    "    loss = criterion(predictions, label.float())\n",
    "    \n",
    "    rounded_preds = torch.round(\n",
    "        torch.sigmoid(predictions))\n",
    "    correct = \\\n",
    "      (rounded_preds == label).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    test_acc += acc.item()\n",
    "\n",
    "print(\"Test: Loss: %.4f Acc: %.4f\" %\n",
    "        (test_loss / len(test_dataloader), \n",
    "        test_acc / len(test_dataloader)))\n",
    "# out: (your results will vary)\n",
    "#   Test: Loss: 0.3821 Acc: 0.8599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1615844842113,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "yncFrpAcD2GY",
    "outputId": "fbd7ff12-c2ce-410c-e8d2-88dfdbb80487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02011590264737606\n",
      "0.8684893846511841\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] \n",
    "      for token in tokenizer(x)]\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    text = torch.tensor(text_pipeline(sentence)).unsqueeze(1).to(device)\n",
    "    prediction = torch.sigmoid(model(text))\n",
    "    return prediction.item()\n",
    "\n",
    "sentiment = predict_sentiment(model, \n",
    "                  \"Don't waste your time\")\n",
    "print(sentiment)\n",
    "# out: 4.763594888613835e-34\n",
    "\n",
    "sentiment = predict_sentiment(model, \n",
    "                  \"You gotta see this movie!\")\n",
    "print(sentiment)\n",
    "# out: 0.941755473613739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "adVgHt83EGbs"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "pJ2CmUZq21-6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/lixiaochuan/Documents/dl/pytorch-book/04_02_Sentiment_Analysis_cnn_new_TorchText_embedding.ipynb Cell 45'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/lixiaochuan/Documents/dl/pytorch-book/04_02_Sentiment_Analysis_cnn_new_TorchText_embedding.ipynb#ch0000034?line=0'>1</a>\u001b[0m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1Y66iHpZIpvg6TX6U00ZP",
   "collapsed_sections": [],
   "name": "04_02_Sentiment_Analysis_with_TorchText.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8d1b3b63f0ab60c7236a713968b959123020c0d6fb9cdd1ffb50b409abe40ad8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
