{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ5SE1D2mFMs"
   },
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "PAG915JevD3i"
   },
   "outputs": [],
   "source": [
    "# For reproducibility\n",
    "import random\n",
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "random_state = random.seed(SEED)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda:1\" if \n",
    "    torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "tokenizer = get_tokenizer('spacy', language='en_core_web_sm') # <1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(torch.cuda.current_device())\n",
    "\n",
    "torch.cuda.set_device(1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 249,
     "status": "ok",
     "timestamp": 1615841842114,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "4u0I1d526CxQ",
    "outputId": "7b3d42b5-37a0-436d-df45-c1d4eda3c973"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['This', 'movie', 'is', 'awesome', 'movie is', 'This movie', 'is awesome']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_bigrams(x):\n",
    "    n_grams = set(zip(*[x[i:] for i in range(2)]))\n",
    "    for n_gram in n_grams:\n",
    "        x.append(' '.join(n_gram))\n",
    "    return x\n",
    "\n",
    "generate_bigrams([\n",
    " 'This', 'movie', 'is', 'awesome'])\n",
    "# out:\n",
    "# ['This', 'movie', 'is', 'awesome', 'This movie',\n",
    "#  'movie is', 'is awesome']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "LLCXyoKT6CxU"
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# from torchtext import data\n",
    "# from torchtext import datasets\n",
    "\n",
    "# TEXT = data.Field(tokenize = 'spacy',\n",
    "#                   preprocessing = \\\n",
    "#                     generate_bigrams) # <1>\n",
    "\n",
    "# LABEL = data.LabelField(dtype = torch.float) # <2>\n",
    "\n",
    "# train_data, test_data = \\\n",
    "#   datasets.IMDB.splits(TEXT, LABEL) # <3>\n",
    "# train_data, valid_data = train_data.split(\n",
    "#     random_state=random_state) # <4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "AEjFcr_aJgIr"
   },
   "outputs": [],
   "source": [
    "from torchtext.datasets import IMDB\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "train_iter, test_iter = IMDB(\n",
    "    split=('train', 'test')) #<1>\n",
    "\n",
    "train_dataset = list(train_iter) #<2>\n",
    "test_dataset  = list(test_iter)\n",
    "\n",
    "num_train = int(len(train_dataset) * 0.70)\n",
    "train_data, valid_data = \\\n",
    "    random_split(train_dataset, \n",
    "        [num_train, \n",
    "         len(train_dataset) - num_train]) # <3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 357,
     "status": "ok",
     "timestamp": 1615841915453,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "7IOq1ufhq01Q",
    "outputId": "ffa9f08a-66ac-4119-a257-c4fd2c035ff5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17500 7500 25000\n",
      "pos\n",
      "I couldn't find anyone to watch DiG! with me because no one I knew was a fan of either of the bands. Naturally everyone assumed you can only enjoy this film if you like the music of either The Dandy Warhols or the Brian Jonestown Massacre, but this is so far from the truth. The only requirement is that you have an interest in music and/or pop culture in general. The way in which the careers of the two groups are paralleled is a perfect representation of the paths a band can take, and watching the public eat up and spit out the Dandy Warhols is fascinating. I agree with other reviews that mention it would be nice to get a final word from Anton himself, since he's clearly depicted as his own worst enemy and the bulwark to the band's ability to just remain.<br /><br />Most interesting to me is the Dandys' respect for the BJM (despite their lack or reciprocation) and for Anton (despite his erratic behavior). The Dandy Warhols respect the art the group produces even if the group hates everything the Dandy Warhols now stand for (although that's disputable). The best line is when the drummer for the Dandy's says \"I won't have them anywhere new me again\" and the guitarist unconsciously blurts out \"I'll still buy their records though.\" To me, this just shows how powerful good music can be.<br /><br />Definitely see this movie, even if you know nothing of either band. It's more about the themes of rock music and how they develop that makes this film so interesting. It's rare to follow a group so closely for so long.\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data), len(valid_data), len(test_dataset))\n",
    "# out:17500 7500 25000\n",
    "\n",
    "data_index = 21\n",
    "print(train_data[data_index][0])\n",
    "# out: (your results may vary)\n",
    "#   pos\n",
    "\n",
    "print(train_data[data_index][1])\n",
    "# out: (your results may vary)\n",
    "# ['This', 'film', 'moved', 'me', 'beyond', 'comprehension', ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.vocab import GloVe\n",
    "embed_len = 200\n",
    "global_vectors = GloVe(name='6B', dim=embed_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2661,  0.2182, -0.1100,  ..., -0.1198, -0.1916, -0.1352],\n",
       "        [ 0.1765,  0.2921, -0.0021,  ..., -0.2077, -0.2319, -0.1081],\n",
       "        [ 0.1815,  0.2663,  0.0550,  ...,  0.5375,  0.3151,  0.0162],\n",
       "        [ 0.0367,  0.1989, -0.0930,  ..., -0.0133, -0.0039,  0.7128],\n",
       "        [ 0.8540,  0.5715, -0.0237,  ...,  0.3108, -0.2230,  0.2037],\n",
       "        [ 0.3911,  0.4019, -0.1505,  ..., -0.0348,  0.0798,  0.5031]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings = global_vectors.get_vecs_by_tokens(tokenizer(\"Hello, How are you?\"), lower_case_backup=True)\n",
    "\n",
    "embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_vectors.get_vecs_by_tokens(\"<BOS>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 239778,
     "status": "ok",
     "timestamp": 1615842157475,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "Sc9_0aaBODCh",
    "outputId": "85e18e96-9ec4-42fa-fe04-478d5e10a886"
   },
   "outputs": [],
   "source": [
    "# this is using the frequencies in the data \n",
    "\n",
    "# https://coderzcolumn.com/tutorials/artificial-intelligence/how-to-use-glove-embeddings-with-pytorch#Load-Glove-'42B'-Embeddings\n",
    "\n",
    "from collections import Counter\n",
    "from torchtext.vocab import vocab\n",
    "from torchtext.vocab import build_vocab_from_iterator \n",
    "# tokenizer(next(train_iter)[1])\n",
    "\n",
    "def yield_tokens(data_iter):\n",
    "    for _, text in data_iter:\n",
    "        yield tokenizer(text)\n",
    "\n",
    "vocab = build_vocab_from_iterator(yield_tokens(train_iter), specials=['<unk>', '<BOS>', '<EOS>', '<PAD>'])\n",
    "        \n",
    "# # create vocabulatory\n",
    "# counter = Counter()\n",
    "# for (label, line) in train_iter:\n",
    "#     counter.update(tokenizer(line))\n",
    "\n",
    "# # be careful, lower case vocab to call function   \n",
    "# vocab = vocab(counter, min_freq=10, specials=('<unk>', '<BOS>', '<EOS>', '<PAD>'))\n",
    "vocab.set_default_index(vocab['<unk>'])  # default index for oov words\n",
    "\n",
    "# why use vocab?\n",
    "\n",
    "# change data from str to numeric \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121069, [164, 11, 45, 493], 'was', 21)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab), vocab(['here', 'is', 'an', 'example']), vocab.lookup_token(21), vocab['was']\n",
    "\n",
    "# THE LENGTH IS MUCH LARGER THAN USING COUNTER, WHICH IS 23404"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0.], device='cuda:1')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create embedding for dat\n",
    "\n",
    "# embed_len = 100\n",
    "\n",
    "glove_embedding_tensor = torch.zeros(len(vocab),  embed_len).to(device)\n",
    " \n",
    "glove_embedding_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(vocab)):\n",
    "    try: \n",
    "        glove_embedding_tensor[i] = global_vectors.get_vecs_by_tokens(vocab.lookup_token(i))\n",
    "    #except KeyError as e:\n",
    "    #    glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "    except KeyError:\n",
    "        glove_embedding_tensor[i] = torch.normal(0, 1, size=(embed_len,))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0., device='cuda:1')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_embedding_tensor[len(vocab)-2].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'â‚¤100'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_token(len(vocab)-2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([121069, 200]),\n",
       " tensor([[-3.7880e-01,  5.1459e-01, -7.3694e-01, -5.6198e-02,  5.2874e-02,\n",
       "           1.5662e-01,  2.1436e-01,  3.4249e-01,  1.4240e-01, -9.3278e-02,\n",
       "           1.6948e-01, -1.1071e-01, -5.6108e-01, -6.0902e-01,  8.6495e-01,\n",
       "           8.8216e-02,  1.9156e-02, -1.8672e-01, -4.5746e-01, -2.9990e-01,\n",
       "           7.6822e-01,  1.9607e+00,  2.0502e-01, -2.4833e-01,  7.9986e-02,\n",
       "           2.2780e-01,  4.6934e-01, -9.3518e-02,  2.9816e-01,  3.3182e-02,\n",
       "           3.8590e-01,  4.7495e-02, -1.7896e-01, -7.4432e-01, -6.6266e-01,\n",
       "          -4.5246e-02, -4.2622e-01, -4.5248e-01,  7.1457e-02, -4.4291e-01,\n",
       "          -4.4779e-01,  4.8310e-03, -7.2398e-01, -1.5956e-01, -3.1243e-01,\n",
       "          -1.8692e-01,  5.8064e-01,  4.1169e-01,  1.1543e-01,  4.9991e-01,\n",
       "           3.8175e-02, -4.4620e-01,  6.4993e-01,  1.1448e+00, -2.4783e-01,\n",
       "          -2.7646e-01, -4.7514e-01,  6.5408e-01, -6.5782e-02, -2.7297e-01,\n",
       "           2.4473e-03, -1.5304e-01, -5.6501e-01, -3.2696e-01,  1.3107e+00,\n",
       "           4.9093e-01, -2.8914e-01,  1.7231e-01,  7.6938e-01, -2.8518e-01,\n",
       "          -2.2873e-01, -5.0518e-01,  6.0937e-01,  1.4541e-01,  3.9290e-01,\n",
       "           5.5956e-02, -5.5703e-01, -3.8893e-02, -2.0495e-01,  5.6423e-01,\n",
       "           8.2132e-03, -6.6767e-02, -2.9927e-01,  6.0117e-01, -6.3384e-02,\n",
       "          -6.4189e-01, -5.5444e-01,  5.0138e-01, -3.6673e-01, -4.9304e-02,\n",
       "          -7.2545e-01, -2.9274e-01, -2.1346e-03,  1.8312e-01,  4.6072e-01,\n",
       "          -7.7038e-01,  2.6467e-01,  3.3044e-01,  2.2858e-01, -4.4268e-01,\n",
       "          -5.4123e-01,  3.8888e-01, -1.4331e-01,  1.1509e-01,  1.2356e-01,\n",
       "          -1.5888e-01,  1.5258e-01,  1.7362e+00, -3.9500e-01, -2.7246e-01,\n",
       "           2.0712e-02, -2.2041e-01, -5.7030e-01,  2.3306e-01, -1.0842e+00,\n",
       "           8.6591e-01, -5.1683e-01,  3.7414e-01,  2.0773e-01,  5.4710e-02,\n",
       "          -3.2593e-01,  5.6176e-01, -3.3873e-02,  5.9226e-01,  2.2097e-01,\n",
       "          -9.9521e-01, -7.0702e-01, -6.0332e-01,  7.5742e-01,  3.7470e-03,\n",
       "          -3.9539e-01, -4.1141e-02,  5.2373e-01, -1.0311e-01,  6.4206e-01,\n",
       "           6.3670e-01, -1.8812e-01,  1.3518e-01, -1.0743e+00,  9.8547e-02,\n",
       "          -3.2638e-02, -1.9948e-01, -3.2529e-01, -1.5256e-01,  6.6934e-01,\n",
       "           5.4685e-01,  2.2348e-01,  6.4717e-03,  7.9020e-02,  8.4018e-01,\n",
       "          -3.7394e-01,  4.0915e-01,  2.0513e-01, -3.7579e-01, -3.8788e-02,\n",
       "          -4.6400e-01, -2.6361e-01,  3.5504e-01, -5.2044e-01, -2.6436e-01,\n",
       "          -4.4091e-01,  2.1041e-01, -1.7172e-01,  5.2400e-02, -2.0947e-01,\n",
       "           4.8091e-02,  3.7248e-01,  2.8542e-01, -2.6875e-01,  2.7937e-01,\n",
       "          -1.2130e-02,  4.2894e-02,  1.9882e-01, -3.1488e-01,  1.0614e-01,\n",
       "           2.7269e-01, -1.0340e+00, -6.1990e-01, -4.6450e-02,  1.3484e-01,\n",
       "           1.2803e+00,  3.4117e-01,  1.5021e-01,  5.1793e-01,  1.0658e-01,\n",
       "          -4.5049e-01,  3.9682e-02,  6.2014e-01,  4.1656e-01, -3.7139e-01,\n",
       "          -2.1261e-01,  4.9009e-01, -2.9024e-01, -4.3822e-02,  3.6139e-01,\n",
       "           8.6226e-01,  4.8318e-01,  4.4144e-03,  1.8841e-01, -3.8341e-01],\n",
       "         [-6.5865e-01,  5.3423e-01,  4.3777e-02,  6.7393e-01,  1.6602e-01,\n",
       "          -4.1741e-01,  3.9818e-01,  5.3211e-01, -3.3676e-01,  7.9238e-01,\n",
       "          -3.1563e-01,  4.4786e-01,  2.5974e-01, -5.9847e-01,  2.0954e-01,\n",
       "           4.5330e-01,  5.9771e-01,  4.0166e-01, -3.2031e-01, -2.5832e-01,\n",
       "           7.7701e-01,  2.9981e+00, -3.5204e-01,  6.2900e-01,  2.8480e-01,\n",
       "          -2.4623e-02,  3.4439e-01, -1.4069e-01, -6.8782e-01,  6.2096e-01,\n",
       "          -5.1506e-01, -7.2010e-01,  3.0794e-01, -3.7843e-01, -4.9103e-01,\n",
       "           1.0848e+00,  4.4457e-01, -4.6116e-02,  1.4095e-01,  1.3767e-01,\n",
       "           2.5804e-02,  2.0366e-01, -2.8700e-02, -1.3788e-01,  2.2944e-02,\n",
       "          -3.2601e-01,  1.3869e-01,  9.5038e-01,  1.4023e-01,  1.3031e-01,\n",
       "          -6.2936e-01, -8.1791e-01,  4.2066e-01,  1.8227e-01, -3.8384e-01,\n",
       "           1.9960e-01,  8.0223e-01, -7.3926e-01,  3.0488e-01,  6.8268e-01,\n",
       "          -2.0069e-02, -1.4479e-01, -1.6923e-01,  1.7878e-01,  6.7149e-01,\n",
       "          -1.8140e-01, -1.0353e+00,  9.8209e-01,  3.5636e-03,  2.8201e-02,\n",
       "           1.4808e-01, -5.8151e-01,  3.3226e-01,  2.2428e-01,  2.0088e-01,\n",
       "           3.8332e-01, -5.3412e-01, -7.2728e-02,  1.5515e-01, -3.3184e-01,\n",
       "           4.3845e-01, -4.9682e-01,  1.2097e-01, -3.4529e-01,  2.9147e-01,\n",
       "          -5.1180e-01, -8.1401e-01,  5.8428e-01,  9.5096e-01,  1.3717e-01,\n",
       "          -1.2384e+00,  1.4919e-01, -4.8114e-02, -1.2080e-01, -1.3829e-01,\n",
       "           5.9211e-02, -8.6004e-01,  9.3794e-01,  1.6802e-01, -5.8304e-01,\n",
       "           8.1835e-01,  1.7855e-01,  3.9457e-01, -1.3119e-01, -3.9576e-01,\n",
       "          -8.1821e-01, -4.4240e-01,  1.0366e+00,  5.1156e-01, -1.5614e-03,\n",
       "           4.7977e-01, -2.0372e-01, -2.2248e-01,  2.7932e-01,  1.0755e-01,\n",
       "           1.7175e-01,  4.9290e-02, -2.1216e-01, -8.2777e-02, -3.7686e-01,\n",
       "           6.0366e-01,  6.1655e-01,  1.4334e-01, -3.5357e-01,  8.0505e-01,\n",
       "          -3.1358e-01,  5.4217e-01, -2.2839e-01, -8.1750e-02,  9.4913e-01,\n",
       "          -3.9255e-02, -3.4487e-01,  5.0564e-01, -4.9076e-01,  4.2899e-01,\n",
       "          -1.9273e-01, -5.1348e-01, -3.2148e-01, -2.9057e-02,  1.1805e-01,\n",
       "           2.1640e-01,  8.1328e-02,  5.9382e-01, -4.0867e-01,  1.7804e+00,\n",
       "           4.6344e-02,  3.2554e-01, -2.6692e-01,  2.4442e-01, -3.9477e-01,\n",
       "           4.8686e-02,  7.6914e-01,  8.0889e-01, -2.7523e-01,  4.0131e-01,\n",
       "          -6.2193e-01, -5.9757e-01,  6.3340e-01, -2.6412e-01, -2.8552e-01,\n",
       "          -5.5509e-01,  4.4657e-01, -7.0154e-01,  1.9491e-01, -4.0020e-01,\n",
       "           2.4529e-01, -1.6255e-01, -1.3351e-01, -5.2555e-01,  9.0975e-01,\n",
       "          -2.8669e-01,  1.0088e-01, -8.8846e-02, -4.1312e-01,  4.6584e-01,\n",
       "          -2.5755e-02, -1.3266e+00, -5.7612e-01, -2.3418e-01,  5.9119e-02,\n",
       "           7.7479e-01,  1.2262e-01,  6.6893e-01, -3.5013e-01, -5.5826e-01,\n",
       "          -3.4336e-01, -1.2027e-01, -2.1892e-01,  4.3901e-01, -5.5061e-01,\n",
       "           2.9896e-02,  1.2844e-01,  1.6361e-01,  2.2208e-01,  7.6550e-03,\n",
       "           8.5989e-01,  2.9140e-01, -1.8455e-01, -5.8256e-01,  4.7415e-01]],\n",
       "        device='cuda:1'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#text_transform = lambda x: [global_vectors.get_vecs_by_tokens('<BOS>')] + [global_vectors.get_vecs_by_tokens(token) for token in tokenizer(x)] + [global_vectors.get_vecs_by_tokens('<EOS>')]\n",
    "\n",
    "#t1 = text_transform(\"how are you?\")\n",
    " \n",
    "glove_embedding_tensor.shape,glove_embedding_tensor[1232:1234,]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "text_transform = lambda x: [vocab['<BOS>']] + [vocab[token] for token in tokenizer(x)] + [vocab['<EOS>']]\n",
    "label_transform = lambda x: 1 if x == 'pos' else 0\n",
    "    \n",
    "def collate_batch(batch):\n",
    "   label_list, text_list = [], []\n",
    "   for (_label, _text) in batch: # each represents one text\n",
    "        label_list.append(label_transform(_label))\n",
    "        processed_text = torch.tensor(text_transform(_text))\n",
    "        text_list.append(processed_text)\n",
    "   return torch.tensor(label_list, dtype=torch.long).to(device), pad_sequence(text_list,  padding_value=1.0).to(device) # not using batch_first=True,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EMy__zqIPbll"
   },
   "outputs": [],
   "source": [
    "\n",
    "BATCH_SIZE = 64\n",
    "train_dataloader = DataLoader(dataset=train_data, \n",
    "                              batch_size=BATCH_SIZE, \n",
    "                              shuffle=True , \n",
    "                              collate_fn=collate_batch)\n",
    " \n",
    "                  # collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(valid_data, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, \n",
    "                  batch_size=BATCH_SIZE,\n",
    "                  shuffle=True, \n",
    "                  collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "EWQrfaT46CxW"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [   13,    13,    13,  ...,    13, 15731, 15562],\n",
       "         [  232,    79,   315,  ...,    58,     5,   832],\n",
       "         ...,\n",
       "         [    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [    1,     1,     1,  ...,     1,     1,     1],\n",
       "         [    1,     1,     1,  ...,     1,     1,     1]], device='cuda:1'),\n",
       " tensor([1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0], device='cuda:1'),\n",
       " 1182,\n",
       " 64,\n",
       " torch.Size([1182, 64]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainlabel, trainfeature = next(iter(train_dataloader)) \n",
    "trainfeature,trainlabel, len(trainfeature), len(trainlabel), trainfeature.shape, # len is by row\n",
    "\n",
    "\n",
    "# 1182 is the longest length of the batch \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rnqlCW9arhLP"
   },
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "YcA1qdwD6CxY"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.conv_0 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[0], embedding_dim))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[1], embedding_dim))\n",
    "        \n",
    "        self.conv_2 = nn.Conv2d(in_channels = 1, \n",
    "                                out_channels = n_filters, \n",
    "                                kernel_size = (filter_sizes[2], embedding_dim))\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len] \n",
    "        # cnn expects batch first, but text shape is reversed \n",
    "        # text.shape [934, 64], label : [64] \n",
    "        embedded  = self.embedding(text) \n",
    "        \n",
    "        embedded = embedded.permute(1, 0, 2)# need to put the batch first \n",
    "\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim] , [64,1,962,200]\n",
    "        \n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3)) \n",
    "        # conv_0 will output tensor of shape [64, 100, 960, 1], 100 - output chanel from 100  filters\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2), dim = 1))\n",
    "        # size :[64,300]\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)   # [64,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([3], 121069)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_indices([\"<PAD>\"]), len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(vocab)\n",
    "EMBEDDING_DIM = 200\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [3,4,5]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = vocab.lookup_indices([\"<PAD>\"])[0]\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "w43pDDgT6CxY"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 24,454,401 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        ...,\n",
       "        [ 0.2642,  0.0694, -0.3210,  ..., -0.0317, -0.1033, -0.0825],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.copy_(glove_embedding_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([121069, 200])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.embedding.weight.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yndecIPP6Cxa"
   },
   "source": [
    "Not forgetting to zero the initial weights of our unknown and padding tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRrHD3Ss6Cxb"
   },
   "source": [
    "## Train & Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "7lpQKr4u-qjA"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 211542,
     "status": "ok",
     "timestamp": 1615844247885,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "B7hBXR41-4HE",
    "outputId": "8dbc4e39-8490-499d-fa90-654d81f60e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 Train: Loss: 0.4493 Acc: 0.7803\n",
      "Epoch 0 Valid: Loss: 0.3536 Acc: 0.8440\n",
      "Epoch 1 Train: Loss: 0.2647 Acc: 0.8924\n",
      "Epoch 1 Valid: Loss: 0.2711 Acc: 0.8901\n",
      "Epoch 2 Train: Loss: 0.1562 Acc: 0.9442\n",
      "Epoch 2 Valid: Loss: 0.2851 Acc: 0.8814\n",
      "Epoch 3 Train: Loss: 0.0685 Acc: 0.9802\n",
      "Epoch 3 Valid: Loss: 0.2921 Acc: 0.8918\n",
      "Epoch 4 Train: Loss: 0.0272 Acc: 0.9941\n",
      "Epoch 4 Valid: Loss: 0.3328 Acc: 0.8880\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(5):\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  \n",
    "  model.train()\n",
    "  for label, text in train_dataloader:\n",
    "      #print(label.dtype)\n",
    "      #print(text)\n",
    "      optimizer.zero_grad()\n",
    "      predictions = model(text).squeeze(1)\n",
    "      #print(predictions.dtype)\n",
    "      loss = criterion(predictions, label.float())\n",
    "      \n",
    "      rounded_preds = torch.round(\n",
    "          torch.sigmoid(predictions))\n",
    "      correct = \\\n",
    "        (rounded_preds == label).float()\n",
    "      acc = correct.sum() / len(correct)\n",
    "      \n",
    "      loss.backward()\n",
    "      optimizer.step()\n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  print(\"Epoch %d Train: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(train_dataloader), \n",
    "          epoch_acc / len(train_dataloader)))\n",
    "\n",
    "  epoch_loss = 0\n",
    "  epoch_acc = 0\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for label, text in valid_dataloader:\n",
    "      predictions = model(text).squeeze(1)\n",
    "      loss = criterion(predictions, label.float())\n",
    "      \n",
    "      rounded_preds = torch.round(\n",
    "          torch.sigmoid(predictions))\n",
    "      correct = \\\n",
    "        (rounded_preds == label).float()\n",
    "      acc = correct.sum() / len(correct)\n",
    "      \n",
    "      epoch_loss += loss.item()\n",
    "      epoch_acc += acc.item()\n",
    "\n",
    "  print(\"Epoch %d Valid: Loss: %.4f Acc: %.4f\" %\n",
    "          (epoch,\n",
    "          epoch_loss / len(valid_dataloader), \n",
    "          epoch_acc / len(valid_dataloader)))\n",
    "  \n",
    "# out: (your results may vary)\n",
    "# Epoch 0 Train: Loss: 0.6523 Acc: 0.7165\n",
    "# Epoch 0 Valid: Loss: 0.5259 Acc: 0.7474\n",
    "# Epoch 1 Train: Loss: 0.5935 Acc: 0.7765\n",
    "# Epoch 1 Valid: Loss: 0.4571 Acc: 0.7933\n",
    "# Epoch 2 Train: Loss: 0.5230 Acc: 0.8257\n",
    "# Epoch 2 Valid: Loss: 0.4103 Acc: 0.8245\n",
    "# Epoch 3 Train: Loss: 0.4559 Acc: 0.8598\n",
    "# Epoch 3 Valid: Loss: 0.3828 Acc: 0.8549\n",
    "# Epoch 4 Train: Loss: 0.4004 Acc: 0.8813\n",
    "# Epoch 4 Valid: Loss: 0.3781 Acc: 0.8675\n",
    "\n",
    "\n",
    "# WHY THE RESULT IS VERY CLOSE TO CREATING VOCAB USING COUNTER\n",
    "# ALTHOUGH THE SIZE OF VOCAB IS VEYR DIFFERENT "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XfBcC4qWDkcS"
   },
   "source": [
    "# Testing & Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41614,
     "status": "ok",
     "timestamp": 1615844337404,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "WdDAlQlaDP0-",
    "outputId": "5a523532-a9b5-4cc4-8803-84c9ad121358"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test: Loss: 0.3456 Acc: 0.8835\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "test_acc = 0\n",
    "model.eval() # <1>\n",
    "with torch.no_grad(): # <1>\n",
    "  for label, text in test_dataloader:\n",
    "    predictions = model(text).squeeze(1)\n",
    "    loss = criterion(predictions, label.float())\n",
    "    \n",
    "    rounded_preds = torch.round(\n",
    "        torch.sigmoid(predictions))\n",
    "    correct = \\\n",
    "      (rounded_preds == label).float()\n",
    "    acc = correct.sum() / len(correct)\n",
    "\n",
    "    test_loss += loss.item()\n",
    "    test_acc += acc.item()\n",
    "\n",
    "print(\"Test: Loss: %.4f Acc: %.4f\" %\n",
    "        (test_loss / len(test_dataloader), \n",
    "        test_acc / len(test_dataloader)))\n",
    "# out: (your results will vary)\n",
    "#   Test: Loss: 0.3821 Acc: 0.8599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 872,
     "status": "ok",
     "timestamp": 1615844842113,
     "user": {
      "displayName": "Joe Papa",
      "photoUrl": "",
      "userId": "00487850786587503652"
     },
     "user_tz": 240
    },
    "id": "yncFrpAcD2GY",
    "outputId": "fbd7ff12-c2ce-410c-e8d2-88dfdbb80487"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.08883300423622131\n",
      "0.5945917963981628\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "\n",
    "text_pipeline = lambda x: [vocab[token] \n",
    "      for token in tokenizer(x)]\n",
    "\n",
    "\n",
    "def predict_sentiment(model, sentence):\n",
    "    model.eval()\n",
    "    text = torch.tensor(text_pipeline(sentence)).unsqueeze(1).to(device)\n",
    "    prediction = torch.sigmoid(model(text))\n",
    "    return prediction.item()\n",
    "\n",
    "sentiment = predict_sentiment(model, \n",
    "                  \"Don't waste your time\")\n",
    "print(sentiment)\n",
    "# out: 4.763594888613835e-34\n",
    "\n",
    "sentiment = predict_sentiment(model, \n",
    "                  \"You gotta see this movie!\")\n",
    "print(sentiment)\n",
    "# out: 0.941755473613739"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "adVgHt83EGbs"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'cnn-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "pJ2CmUZq21-6"
   },
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyM1Y66iHpZIpvg6TX6U00ZP",
   "collapsed_sections": [],
   "name": "04_02_Sentiment_Analysis_with_TorchText.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "8d1b3b63f0ab60c7236a713968b959123020c0d6fb9cdd1ffb50b409abe40ad8"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
